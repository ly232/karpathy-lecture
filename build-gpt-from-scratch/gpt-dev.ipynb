{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d75eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d8482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'length of text: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f51e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f90a0b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0616ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# tokenize: maps tokens to integers\n",
    "stoi = {s: i for i, s in enumerate(chars)}\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "encode = lambda sentence: [stoi[c] for c in sentence]\n",
    "decode = lambda tokens: ''.join([itos[i] for i in tokens])\n",
    "\n",
    "print(encode('hello world'))\n",
    "print(decode(encode('hello world')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d00ce",
   "metadata": {},
   "source": [
    "## Other popular encoders:\n",
    "* [SentencePiece](https://github.com/google/sentencepiece) (Google)\n",
    "* [TikToken](https://github.com/openai/tiktoken) (OpenAI)\n",
    "\n",
    "These take more bytes per token, so less number of tokens per sentence. They\n",
    "also have larger vocab size given it's trained over larger datasets.\n",
    "\n",
    "Side note on installing packages:\n",
    "* Prefer to install within uv's venv.\n",
    "* Also need to hook to Jupyter kernel. This needs (a) ctrl+shift+P in VSCode to\n",
    "  select python kernel to venv, and (b) in upper-right corner, change kernel to\n",
    "  point to uv's venv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e5fd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373, 995]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "enc.n_vocab\n",
    "print(enc.encode('hello world'))\n",
    "print(enc.decode([31373, 995]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf91f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])\n",
    "\n",
    "# text is a list of characters (1115394 chars)\n",
    "# encode(text) is a list of ints (1115394 ints)\n",
    "# torch.tensor(encode(text)) transforms into a 1-d tensor (shape == (1115394,))\n",
    "assert data.numpy().shape == (1115394,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a8c6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / validation data split\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92a62d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transformer training\n",
    "# Never feed entire corpus to transformer in one go.\n",
    "# Sample random chunks to train.\n",
    "# A chucnk has max length, `block_size`.\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2bafce",
   "metadata": {},
   "source": [
    "Taking `[18, 47, 56, 57, 58,  1, 15, 47, 58]` as an example:\n",
    "* In the context of `[]`, `18` comes next.\n",
    "* In the context of [`18`], `47` comes next.\n",
    "* In the context of [`18`, `47`], `56` comes next.\n",
    "* And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71d784b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), target is 47\n",
      "when input is tensor([18, 47]), target is 56\n",
      "when input is tensor([18, 47, 56]), target is 57\n",
      "when input is tensor([18, 47, 56, 57]), target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context}, target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad5756",
   "metadata": {},
   "source": [
    "We trained all O(block_size) number of samples (context length = 1 ... 8).\n",
    "* This is not just for computational efficiency.\n",
    "* Also useful for transformer to see context of different lengths.\n",
    "* Crucial for inference, because when we start sampling, a single token can\n",
    "  start the generation process because training has seen len-1 and len-n context\n",
    "  windows during training. \n",
    "* After `block_size`, we'll have to start truncating, because transformer never\n",
    "  sees more than `block_size` inputs when it predicts next token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94b03a",
   "metadata": {},
   "source": [
    "Now we introduce a new dimension: batch (in addition to time dimension).\n",
    "* As we sample chunks of text, every time we feed into a transformer, we have\n",
    "  multiple mini batches of multiple chunks of texts, **stacked up in a single tensor**.\n",
    "* This is just for efficiency to keep GPUs busy. **Each batch is processed independently**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15d7d36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "when input is tensor([24]), output is 43\n",
      "when input is tensor([24, 43]), output is 58\n",
      "when input is tensor([24, 43, 58]), output is 5\n",
      "when input is tensor([24, 43, 58,  5]), output is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]), output is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]), output is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]), output is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]), output is 39\n",
      "when input is tensor([44]), output is 53\n",
      "when input is tensor([44, 53]), output is 56\n",
      "when input is tensor([44, 53, 56]), output is 1\n",
      "when input is tensor([44, 53, 56,  1]), output is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]), output is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]), output is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]), output is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]), output is 1\n",
      "when input is tensor([52]), output is 58\n",
      "when input is tensor([52, 58]), output is 1\n",
      "when input is tensor([52, 58,  1]), output is 58\n",
      "when input is tensor([52, 58,  1, 58]), output is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]), output is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]), output is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]), output is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]), output is 46\n",
      "when input is tensor([25]), output is 17\n",
      "when input is tensor([25, 17]), output is 27\n",
      "when input is tensor([25, 17, 27]), output is 10\n",
      "when input is tensor([25, 17, 27, 10]), output is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]), output is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]), output is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]), output is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]), output is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # max context window.\n",
    "\n",
    "def get_batch(split):\n",
    "    # generates a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # ix is the starting index of the context window; generate `batch_size` such\n",
    "    # indexes in parallel, 1 per batch.\n",
    "    # ix is a 1-d tensor of `block_size` elements in 1st dimension.\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # Let's say x == [1, 2, 3], then in a single batch, we have 3 samples (aka 3\n",
    "    # context windows), and for each, the expected target is the next element:\n",
    "    # [1] -> 2\n",
    "    # [1, 2] -> 3\n",
    "    # [1, 2, 3] -> 4\n",
    "    #\n",
    "    # From x's perspective, [data[i:i+block_size] for i in ix] is a 2d tensor,\n",
    "    # but it really contains 3-dimensional information:\n",
    "    # * sampling start index, randomized via torch.randint (1st tensor dim)\n",
    "    # * context window length, varies from 1 to block_size (2nd tensor dim)\n",
    "    # * sub-window per context length (not explicitly a tensor dim)\n",
    "    #\n",
    "    # IOW, each element in the 2d x tensor is an *array* representing\n",
    "    # *full* context window. This single element can be expanded to exactly\n",
    "    # block_size sub context windows aka sub arrays.\n",
    "    #\n",
    "    # From y's perspective, it's also a 2d tensor, but each element is a scalar\n",
    "    # not an array. i-th scalar in y is the target for a sub context window.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('targets')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):  # batch dim\n",
    "    for t in range(block_size):  # time dim\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'when input is {context}, output is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89e0ff",
   "metadata": {},
   "source": [
    "For 1st batch `x=[24, 43, 58,  5, 57,  1, 46, 43]` and `y=[43, 58,  5, 57,  1, 46, 43, 39]`:\n",
    "* input `[24]` targets `[43]`\n",
    "* input `[24, 43]` targets `[58]`\n",
    "* input `[24, 43, 58]` targets `[5]`\n",
    "* ...\n",
    "* input `[24, 43, 58,  5, 57,  1, 46, 43]` targets `[39]`\n",
    "\n",
    "Terminologies:\n",
    "* `Chunk`: largest context window with `block_size` tokens.\n",
    "* `Context window`: 1 or more contigeous tokens as input.\n",
    "* `Target`: a single token.\n",
    "* `Batch`:  a set of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b406bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (karpathy-lecture)",
   "language": "python",
   "name": "karpathy-lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
