{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d75eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d8482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'length of text: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f51e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f90a0b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0616ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# tokenize: maps tokens to integers\n",
    "stoi = {s: i for i, s in enumerate(chars)}\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "encode = lambda sentence: [stoi[c] for c in sentence]\n",
    "decode = lambda tokens: ''.join([itos[i] for i in tokens])\n",
    "\n",
    "print(encode('hello world'))\n",
    "print(decode(encode('hello world')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d00ce",
   "metadata": {},
   "source": [
    "## Other popular encoders:\n",
    "* [SentencePiece](https://github.com/google/sentencepiece) (Google)\n",
    "* [TikToken](https://github.com/openai/tiktoken) (OpenAI)\n",
    "\n",
    "These take more bytes per token, so less number of tokens per sentence. They\n",
    "also have larger vocab size given it's trained over larger datasets.\n",
    "\n",
    "Side note on installing packages:\n",
    "* Prefer to install within uv's venv.\n",
    "* Also need to hook to Jupyter kernel. This needs (a) ctrl+shift+P in VSCode to\n",
    "  select python kernel to venv, and (b) in upper-right corner, change kernel to\n",
    "  point to uv's venv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e5fd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373, 995]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "enc.n_vocab\n",
    "print(enc.encode('hello world'))\n",
    "print(enc.decode([31373, 995]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf91f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])\n",
    "\n",
    "# text is a list of characters (1115394 chars)\n",
    "# encode(text) is a list of ints (1115394 ints)\n",
    "# torch.tensor(encode(text)) transforms into a 1-d tensor (shape == (1115394,))\n",
    "assert data.numpy().shape == (1115394,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a8c6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / validation data split\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92a62d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transformer training\n",
    "# Never feed entire corpus to transformer in one go.\n",
    "# Sample random chunks to train.\n",
    "# A chucnk has max length, `block_size`.\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2bafce",
   "metadata": {},
   "source": [
    "Taking `[18, 47, 56, 57, 58,  1, 15, 47, 58]` as an example:\n",
    "* In the context of `[]`, `18` comes next.\n",
    "* In the context of [`18`], `47` comes next.\n",
    "* In the context of [`18`, `47`], `56` comes next.\n",
    "* And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71d784b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), target is 47\n",
      "when input is tensor([18, 47]), target is 56\n",
      "when input is tensor([18, 47, 56]), target is 57\n",
      "when input is tensor([18, 47, 56, 57]), target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context}, target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad5756",
   "metadata": {},
   "source": [
    "We trained all O(block_size) number of samples (context length = 1 ... 8).\n",
    "* This is not just for computational efficiency.\n",
    "* Also useful for transformer to see context of different lengths.\n",
    "* Crucial for inference, because when we start sampling, a single token can\n",
    "  start the generation process because training has seen len-1 and len-n context\n",
    "  windows during training. \n",
    "* After `block_size`, we'll have to start truncating, because transformer never\n",
    "  sees more than `block_size` inputs when it predicts next token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94b03a",
   "metadata": {},
   "source": [
    "Now we introduce a new dimension: batch (in addition to time dimension).\n",
    "* As we sample chunks of text, every time we feed into a transformer, we have\n",
    "  multiple mini batches of multiple chunks of texts, **stacked up in a single tensor**.\n",
    "* This is just for efficiency to keep GPUs busy. **Each batch is processed independently**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15d7d36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "when input is tensor([24]), output is 43\n",
      "when input is tensor([24, 43]), output is 58\n",
      "when input is tensor([24, 43, 58]), output is 5\n",
      "when input is tensor([24, 43, 58,  5]), output is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]), output is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]), output is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]), output is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]), output is 39\n",
      "when input is tensor([44]), output is 53\n",
      "when input is tensor([44, 53]), output is 56\n",
      "when input is tensor([44, 53, 56]), output is 1\n",
      "when input is tensor([44, 53, 56,  1]), output is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]), output is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]), output is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]), output is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]), output is 1\n",
      "when input is tensor([52]), output is 58\n",
      "when input is tensor([52, 58]), output is 1\n",
      "when input is tensor([52, 58,  1]), output is 58\n",
      "when input is tensor([52, 58,  1, 58]), output is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]), output is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]), output is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]), output is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]), output is 46\n",
      "when input is tensor([25]), output is 17\n",
      "when input is tensor([25, 17]), output is 27\n",
      "when input is tensor([25, 17, 27]), output is 10\n",
      "when input is tensor([25, 17, 27, 10]), output is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]), output is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]), output is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]), output is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]), output is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # max context window.\n",
    "\n",
    "def get_batch(split):\n",
    "    # generates a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # ix is the starting index of the context window; generate `batch_size` such\n",
    "    # indexes in parallel, 1 per batch.\n",
    "    # ix is a 1-d tensor of `block_size` elements in 1st dimension.\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # Let's say x == [1, 2, 3], then in a single batch, we have 3 samples (aka 3\n",
    "    # context windows), and for each, the expected target is the next element:\n",
    "    # [1] -> 2\n",
    "    # [1, 2] -> 3\n",
    "    # [1, 2, 3] -> 4\n",
    "    #\n",
    "    # From x's perspective, [data[i:i+block_size] for i in ix] is a 2d tensor,\n",
    "    # but it really contains 3-dimensional information:\n",
    "    # * sampling start index, randomized via torch.randint (1st tensor dim)\n",
    "    # * context window length, varies from 1 to block_size (2nd tensor dim)\n",
    "    # * sub-window per context length (not explicitly a tensor dim)\n",
    "    #\n",
    "    # IOW, each element in the 2d x tensor is an *array* representing\n",
    "    # *full* context window. This single element can be expanded to exactly\n",
    "    # block_size sub context windows aka sub arrays.\n",
    "    #\n",
    "    # From y's perspective, it's also a 2d tensor, but each element is a scalar\n",
    "    # not an array. i-th scalar in y is the target for a sub context window.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('targets')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):  # batch dim\n",
    "    for t in range(block_size):  # time dim\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'when input is {context}, output is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff89e0ff",
   "metadata": {},
   "source": [
    "For 1st batch `x=[24, 43, 58,  5, 57,  1, 46, 43]` and `y=[43, 58,  5, 57,  1, 46, 43, 39]`:\n",
    "* input `[24]` targets `[43]`\n",
    "* input `[24, 43]` targets `[58]`\n",
    "* input `[24, 43, 58]` targets `[5]`\n",
    "* ...\n",
    "* input `[24, 43, 58,  5, 57,  1, 46, 43]` targets `[39]`\n",
    "\n",
    "Terminologies:\n",
    "* `Chunk`: largest context window with `block_size` tokens.\n",
    "* `Context window`: 1 or more contigeous tokens as input.\n",
    "* `Target`: a single token.\n",
    "* `Batch`:  a set of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b406bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# generate text. start with token index 0 which is newline. batch size 1.\u001b[39;00m\n\u001b[32m     52\u001b[39m idx = torch.zeros((\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), dtype=torch.long)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28mprint\u001b[39m(decode(\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].tolist()))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mBigramLanguageModel.generate\u001b[39m\u001b[34m(self, idx, max_new_tokens)\u001b[39m\n\u001b[32m     35\u001b[39m logits, _ = \u001b[38;5;28mself\u001b[39m(idx)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# focus only on the last time step\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m logits = \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# becomes (B, C)\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# apply softmax to get probabilities\u001b[39;00m\n\u001b[32m     39\u001b[39m probs = F.softmax(logits, dim=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B, C)\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Bi-gram language model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a\n",
    "        # lookup table.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensor of ints.\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C), C=num channels=vocab_size\n",
    "        \n",
    "        # logits is one round of inference. we can then evaluate loss.\n",
    "        # small quirk: pytorch wants channel dimension to come as 2nd dimension,\n",
    "        # i.e. (B*T, C), not (B, T, C).\n",
    "        B, T, C = logits.shape\n",
    "        if targets is not None:\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits, None\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indicies of the current context.\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# generate text. start with token index 0 which is newline. batch size 1.\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c8a4e",
   "metadata": {},
   "source": [
    "Back-of-envolope calculations:\n",
    "* Loss currently is about 4.88\n",
    "* Vocab size is 65\n",
    "* -ln(65) == 4.17 < 4.88\n",
    "* This is saying our initial predictions are not super diffused, got more\n",
    "  entropy, so we're guessing wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1416fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (karpathy-lecture)",
   "language": "python",
   "name": "karpathy-lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
